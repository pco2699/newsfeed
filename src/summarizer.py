"""
AI Summarizer using Claude Haiku 4.5
Categorizes and summarizes articles in Japanese
"""

import anthropic
import os
import logging
from typing import List, Dict, Any
import json

logger = logging.getLogger(__name__)


class AISummarizer:
    """Summarizes and categorizes articles using Claude AI"""

    def __init__(
        self,
        model: str = "claude-haiku-4-20250110",
        max_tokens: int = 4000,
        temperature: float = 0.3
    ):
        """
        Initialize AI summarizer

        Args:
            model: Claude model to use
            max_tokens: Maximum tokens for response
            temperature: Temperature for generation
        """
        self.model = model
        self.max_tokens = max_tokens
        self.temperature = temperature

        # Initialize Anthropic client
        api_key = os.getenv('ANTHROPIC_API_KEY')
        if not api_key:
            raise ValueError("ANTHROPIC_API_KEY environment variable not set")

        self.client = anthropic.Anthropic(api_key=api_key)
        logger.info(f"AI Summarizer initialized with model: {model}")

    def summarize_articles(self, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Summarize and categorize articles

        Args:
            articles: List of article dictionaries from all sources

        Returns:
            Dictionary with categorized summaries and highlights
        """
        logger.info(f"Summarizing {len(articles)} articles...")

        # Prepare article data for the prompt
        article_list = self._format_articles_for_prompt(articles)

        # Create the prompt
        prompt = self._create_summary_prompt(article_list, len(articles))

        try:
            # Call Claude API
            response = self.client.messages.create(
                model=self.model,
                max_tokens=self.max_tokens,
                temperature=self.temperature,
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            )

            # Parse the response
            summary_text = response.content[0].text
            result = self._parse_summary_response(summary_text, articles)

            logger.info(f"Successfully generated summary with {len(result.get('categories', []))} categories")
            return result

        except Exception as e:
            logger.error(f"Error generating summary: {e}")
            # Return a basic fallback structure
            return self._create_fallback_summary(articles)

    def _format_articles_for_prompt(self, articles: List[Dict[str, Any]]) -> str:
        """
        Format articles for the AI prompt

        Args:
            articles: List of article dictionaries

        Returns:
            Formatted string of articles
        """
        lines = []
        for i, article in enumerate(articles, 1):
            source = article.get('source', 'Unknown')
            title = article.get('title', 'No title')
            url = article.get('url', '')
            score = article.get('score', 0)
            score_label = article.get('score_label', f'{score}')

            lines.append(f"{i}. [{source}] {title} ({score_label}) - {url}")

        return "\n".join(lines)

    def _create_summary_prompt(self, article_list: str, count: int) -> str:
        """
        Create the prompt for Claude

        Args:
            article_list: Formatted article list
            count: Number of articles

        Returns:
            Complete prompt string
        """
        return f"""ã‚ãªãŸã¯æ—¥æœ¬èªã§ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã‚’ä½œæˆã™ã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®{count}ä»¶ã®è¨˜äº‹ã‚’åˆ†æã—ã€ã‚½ãƒ¼ã‚¹åˆ¥ï¼ˆã¯ã¦ãƒ–ã€Hacker Newsã€Redditï¼‰ã«åˆ†ã‘ã¦è¦ç´„è¨˜äº‹ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

**é‡è¦ãªæŒ‡ç¤º:**
1. **ã‚½ãƒ¼ã‚¹åˆ¥ã«åˆ†å‰²**: ã¯ã¦ãƒ–ã€Hacker Newsã€Redditã®3ã¤ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«åˆ†ã‘ã¦è¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„
2. **å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æ§‹æˆ**:
   - å„ã‚½ãƒ¼ã‚¹ã”ã¨ã«300-500æ–‡å­—ç¨‹åº¦ã®æ®µè½å½¢å¼ã®è¦ç´„
   - ãã®ã‚½ãƒ¼ã‚¹ã®ä¸»è¦ãªãƒˆãƒ”ãƒƒã‚¯ã‚„ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’èª¬æ˜
   - è¨˜äº‹ã¸ã®ãƒªãƒ³ã‚¯ã‚’è‡ªç„¶ã«åŸ‹ã‚è¾¼ã‚€ï¼ˆä¾‹: ã€Œ[è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«](URL)ã§ã¯...ã€ï¼‰
3. **å…¨ä½“ã‚µãƒãƒªãƒ¼**: å†’é ­ã«å…¨ã‚½ãƒ¼ã‚¹ã‚’é€šã—ã¦ã®ä»Šæ—¥ã®æ³¨ç›®ç‚¹ã‚’2-3æ®µè½ï¼ˆ200-300æ–‡å­—ï¼‰ã§è¨˜è¿°
4. **è‹±èªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç¿»è¨³**: è‹±èªã®è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚„å†…å®¹ã¯è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„
5. **ä¸­ç«‹çš„ãªãƒˆãƒ¼ãƒ³**: å®¢è¦³çš„ã§æƒ…å ±çš„ãªãƒˆãƒ¼ãƒ³ã‚’ä¿ã¡ã€åˆ†æçš„ãªè¦–ç‚¹ã‚’åŠ ãˆã¦ãã ã•ã„
6. **ãƒªãƒ³ã‚¯ã®æŒ¿å…¥**: Markdownå½¢å¼ `[ã‚¿ã‚¤ãƒˆãƒ«](URL)` ã§å¿…ãšè¨˜äº‹ãƒªãƒ³ã‚¯ã‚’å«ã‚ã¦ãã ã•ã„

**å‡ºåŠ›å½¢å¼:**
JSONå½¢å¼ã§ä»¥ä¸‹ã®æ§‹é€ ã§å‡ºåŠ›ã—ã¦ãã ã•ã„:

```json
{{
  "title": "ä»Šæ—¥ã®ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆã®ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆ20æ–‡å­—ä»¥å†…ï¼‰",
  "overall_summary": "å…¨ä½“çš„ãªä»Šæ—¥ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚„æ³¨ç›®ç‚¹ï¼ˆ200-300æ–‡å­—ã€Markdownå½¢å¼ï¼‰",
  "source_summaries": [
    {{
      "source": "ã¯ã¦ãƒ–",
      "icon": "ğŸ“‘",
      "summary": "ã¯ã¦ãƒ–ã‹ã‚‰ã®è¨˜äº‹ã®è¦ç´„ï¼ˆ300-500æ–‡å­—ã€ãƒªãƒ³ã‚¯ä»˜ãMarkdownå½¢å¼ï¼‰",
      "article_count": è¨˜äº‹æ•°
    }},
    {{
      "source": "Hacker News",
      "icon": "ğŸ”¶",
      "summary": "Hacker Newsã‹ã‚‰ã®è¨˜äº‹ã®è¦ç´„ï¼ˆ300-500æ–‡å­—ã€ãƒªãƒ³ã‚¯ä»˜ãMarkdownå½¢å¼ï¼‰",
      "article_count": è¨˜äº‹æ•°
    }},
    {{
      "source": "Reddit",
      "icon": "ğŸ¤–",
      "summary": "Redditã‹ã‚‰ã®è¨˜äº‹ã®è¦ç´„ï¼ˆ300-500æ–‡å­—ã€ãƒªãƒ³ã‚¯ä»˜ãMarkdownå½¢å¼ï¼‰",
      "article_count": è¨˜äº‹æ•°
    }}
  ],
  "key_topics": [
    {{
      "topic": "ãƒˆãƒ”ãƒƒã‚¯å",
      "icon": "é©åˆ‡ãªçµµæ–‡å­—"
    }}
  ],
  "total_articles": {count}
}}
```

**è¨˜äº‹ãƒªã‚¹ãƒˆ:**
{article_list}

å¿…ãšJSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚å„ã‚½ãƒ¼ã‚¹ã®summaryãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ã¯ã€è¨˜äº‹ã¸ã®ãƒªãƒ³ã‚¯ã‚’å«ã‚€æ®µè½å½¢å¼ã®æ—¥æœ¬èªè¦ç´„ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚"""

    def _parse_summary_response(
        self,
        response_text: str,
        original_articles: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Parse Claude's response into structured data

        Args:
            response_text: Response from Claude
            original_articles: Original article data

        Returns:
            Structured summary dictionary
        """
        try:
            # Try to extract JSON from response
            # Claude might wrap JSON in markdown code blocks
            response_text = response_text.strip()

            # Remove markdown code blocks if present
            if response_text.startswith('```json'):
                response_text = response_text[7:]
            if response_text.startswith('```'):
                response_text = response_text[3:]
            if response_text.endswith('```'):
                response_text = response_text[:-3]

            response_text = response_text.strip()

            # Parse JSON
            result = json.loads(response_text)

            # Validate structure for new format
            if 'overall_summary' not in result or 'source_summaries' not in result:
                logger.warning("Missing required fields in response")
                return self._create_fallback_summary(original_articles)

            if 'title' not in result:
                result['title'] = 'ä»Šæ—¥ã®ãƒ†ãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ'

            if 'key_topics' not in result:
                result['key_topics'] = []

            if 'total_articles' not in result:
                result['total_articles'] = len(original_articles)

            return result

        except json.JSONDecodeError as e:
            logger.error(f"Error parsing JSON response: {e}")
            logger.debug(f"Response text: {response_text[:500]}")
            return self._create_fallback_summary(original_articles)

    def _create_fallback_summary(self, articles: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Create a basic fallback summary if AI fails

        Args:
            articles: Original articles

        Returns:
            Basic summary structure
        """
        logger.warning("Creating fallback summary")

        # Sort by score
        sorted_articles = sorted(articles, key=lambda x: x.get('score', 0), reverse=True)

        # Group by source
        by_source = {}
        for article in sorted_articles:
            source = article.get('source', 'ãã®ä»–')
            if source not in by_source:
                by_source[source] = []
            by_source[source].append(article)

        # Create overall summary
        overall_summary = f"ä»Šæ—¥ã¯å…¨ä½“ã§{len(articles)}ä»¶ã®è¨˜äº‹ã‚’åé›†ã—ã¾ã—ãŸã€‚\n\nä¸»ãªãƒˆãƒ”ãƒƒã‚¯ã¯ã€ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã€AIã€ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãªã©ã§ã™ã€‚"

        # Create source summaries
        source_summaries = []
        source_icons = {
            'ã¯ã¦ãƒ–': 'ğŸ“‘',
            'Hacker News': 'ğŸ”¶',
            'Reddit': 'ğŸ¤–'
        }

        for source, source_articles in by_source.items():
            summary_parts = []
            for i, article in enumerate(source_articles[:10], 1):
                title = article.get('title', '')
                url = article.get('url', '')
                summary_parts.append(f"[{title}]({url})")
                if i < len(source_articles[:10]):
                    summary_parts.append("ã€")

            summary_text = f"{source}ã‹ã‚‰ã¯{len(source_articles)}ä»¶ã®è¨˜äº‹ãŒã‚ã‚Šã¾ã™ã€‚ä¸»ãªè¨˜äº‹: " + "".join(summary_parts)

            source_summaries.append({
                'source': source,
                'icon': source_icons.get(source, 'ğŸ“°'),
                'summary': summary_text,
                'article_count': len(source_articles)
            })

        return {
            'title': 'ä»Šæ—¥ã®ãƒ†ãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ',
            'overall_summary': overall_summary,
            'source_summaries': source_summaries,
            'key_topics': [
                {'topic': 'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼', 'icon': 'ğŸ’»'},
                {'topic': 'ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'icon': 'ğŸ“°'}
            ],
            'total_articles': len(articles)
        }

    def _get_source_icon(self, source: str) -> str:
        """Get icon emoji for source"""
        icons = {
            'ã¯ã¦ãƒ–': 'ğŸ“‘',
            'Hacker News': 'ğŸ”¶',
            'Reddit': 'ğŸ¤–'
        }
        return icons.get(source, 'ğŸ“°')
